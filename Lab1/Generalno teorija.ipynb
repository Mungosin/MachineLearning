{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "# ako je u kodu Image(\"Lab1-3b.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Osnovni koncepti\n",
    "\n",
    "### Induktivna pristranost\n",
    "* The decision tree ID3 algorithm searches the complete hypothesis space, and there is no restriction on the number of hypthotheses that could eventually be enumerated. However, this algorithm searches incompletely through the set of possibly hypotheses and preferentially selects those hypotheses that lead to a smaller decision tree. This type of bias is called a **preference (or search) bias**.\n",
    "* In contrast, the version space candidate-elimination algorithm searches through only a subset of the possible hypotheses (an incomplete hypothesis space), yet searches this space completely. This type of bias is called a **restriction (or language) bias**, because the number of possible hyptheses considered is restricted.\n",
    "* A preference bias is generally more desirable than a restriction bias, because an algorithm with this bias is allowed to search through the complete hypothesis space, which is guaranteed to contain the target function.\n",
    "Restricting the hypothesis space being searched (a restriction bias) is less desirable because the target function may not be within the set of hypotheses considered.  \n",
    "**(source -> http://www2.cs.uregina.ca/~dbd/cs831/notes/ml/2_inference.html)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. c) \n",
    "Prikažite primjere iz $\\mathcal{D}$ i funkciju $h(\\tilde{\\mathbf{x}})=\\mathbf{w}^\\intercal\\tilde{\\mathbf{x}}$. Izračunajte pogrešku učenja prema izrazu $E(h|\\mathcal{D})=\\frac{1}{2}\\sum_{i=1}^N(\\tilde{\\mathbf{x}}^{(i)} - h(\\tilde{\\mathbf{x}}))^2$. Možete koristiti funkciju srednje kvadratne pogreške [`mean_squared_error`]( http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error) iz modula [`sklearn.metrics`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Q:** Gore definirana funkcija pogreške $E(h|\\mathcal{D})$ i funkcija srednje kvadrante pogreške nisu posve identične. U čemu je razlika? Koja je funkcija korisnija u praksi? Zašto?\n",
    "\n",
    "**R:** \n",
    "\n",
    "* **Funkcija pogreške** $E(h|\\mathcal{D})$ definirana u prethodnom primjeru je dijeljena sa 2 jer se pretpostavlja jednolika pogreška u pozitivnom i negativnom smjeru. Ona ima baš fizikalno značenje jer možemo prosječno reći koliko po primjeru griješimo\n",
    "\n",
    "*  **Funkcija srednje kvadrante pogreške** svu akumuliranu pogrešku dijeli sa brojem primjera kako bi dobila srednju ukupnu pogrešku na određenom primjeru te ih nakon toga sumira. Nema toliko fizikalno značenje koliko olakšava račun prilikom optimizacije parametara modela"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.d)\n",
    "Uvjerite se da za primjere iz $\\mathcal{D}$ težine $\\mathbf{w}$ ne možemo naći rješavanjem sustava $\\mathbf{w}=\\mathbf{\\Phi}^{-1}\\mathbf{y}$, već da nam doista treba pseudoinverz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Zašto je to slučaj? Bi li se problem mogao riješiti preslikavanjem primjera u višu dimenziju? Ako da, bi li to uvijek funkcioniralo, neovisno o skupu primjera $\\mathcal{D}$? Pokažite na primjeru.\n",
    "\n",
    "**R:** Matrica nije kvadratna pa nije moguce napraviti inverz njezin. Ukoliko se matricu pretvori u kvadratnu dodavanjem novih značajki, tj. preslikavanjem primjera u višu dimenziju. Primjeri se mogu preslikati u područje više dimezije, ali je opet diskutabilno dali bi matrica imala inverz. Moguće je da je sustav nekonzistentan te ovisi o rangu matrice.\n",
    "\n",
    "* Ne bi funkcioniralo za sve primjere. Funkcioniralo bi samo za ovaj naš primjer i svaki drugi set od 4 primjera. Matrica X bi imala inverz samo kada je regularna kvadratna matrica te ukoliko je dimenzija matrice $N\\times (n+1)$\n",
    "\n",
    "* U našem slučaju, imamo $n = 1$ i $N = 4$, kako bi zadovoljili jednadžbu, trebamo generirati dizajn matricu koja će imati 3 značajki te jednu dummy vrijednost kako bi matrica bila kvadratna. Tada možemo napraviti inverz i množiti je sa w vektorom."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. a)\n",
    "\n",
    "Na skupu podataka iz zadatka 2 trenirajte pet modela linearne regresije $\\mathcal{H}_d$ različite složenosti, gdje je $d$ stupanj polinoma, $d\\in\\{1,3,5,10,20\\}$. Prikažite na istome grafikonu skup za učenje i funkcije $h_d(\\mathbf{x})$ za svih pet modela (preporučujemo koristiti `plot` unutar `for` petlje). Izračunajte pogrešku učenja svakog od modela."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Lab1-3a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Koji model ima najmanju pogrešku učenja i zašto?\n",
    "\n",
    "**R:** Najmanju pogrešku ima model sa stupnjem polinoma **d = 20** jer se najviše može adaprirati našem ulaznom skupu podataka. Budući da znamo da je skup podataka za treniranje generiran sa polinomom 3. stupnja, možemo reć da je ovaj model **prenaučen**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. b)\n",
    "Razdvojite skup primjera iz zadatka 2 pomoću funkcije [`cross_validation.train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split) na skup za učenja i skup za ispitivanje u omjeru 1:1. <br>\n",
    "Prikažite na jednom grafikonu **pogrešku učenja** i **ispitnu pogrešku** za modele polinomijalne regresije $\\mathcal{H}_d$, sa stupnjem polinoma $d$ u rasponu $d\\in\\{1,20\\}$. Radi preciznosti, funkcije $h(\\mathbf{x})$ iscrtajte na cijelom skupu primjera (ali pogrešku generalizacije računajte, naravno, samo na ispitnome skupu). Budući da kvadratna pogreška brzo raste za veće stupnjeve polinoma, umjesto da iscrtate izravno iznose pogrešaka, iscrtajte njihove logaritme.\n",
    "\n",
    "**NB:** Podjela na skupa za učenje i skup za ispitivanje mora za svih pet modela biti identična."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Lab1-3b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Je li rezultat u skladu s očekivanjima? Koji biste model odabrali i zašto?\n",
    "\n",
    "**R:** Rezultati su u skladu sa očekivanjima i odabrao bi model složenosti $d = 3$ jer se tamo otprilike uvijek nalazi minimum moje ispitne pogreške.\n",
    "\n",
    "**Q:** Pokrenite iscrtavanje više puta. U čemu je problem? Bi li problem bio jednako izražen kad bismo imali više primjera? Zašto?\n",
    "\n",
    "**R:** Problem je radi nasumičnog odabira jer je moguće da se odaberu male grupice primjera pa da nije **ravnomjerno distribuiran** set za učenje. Zbog toga dolazi do naglih propada i špiceva. Što ravnomjerniji odabiri iz skupa primjera, to su dobiveni grafovi glađi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. d)\n",
    "Točnost modela ovisi o \n",
    "\n",
    "* njegovoj složenosti (stupanj $d$ polinoma), \n",
    "* broju primjera $N$,\n",
    "* količini šuma. \n",
    "\n",
    "Kako biste to analizirali, nacrtajte **grafikone pogrešaka** kao u 3b, ali za sve kombinacija broja primjera $N\\in\\{100,200,1000\\}$ i količine šuma $\\sigma\\in\\{100,200,500\\}$ (ukupno 9 grafikona). Upotrijebite funkciju [`subplots`](http://matplotlib.org/examples/pylab_examples/subplots_demo.html) kako biste pregledno posložili grafikone u tablicu $3\\times 3$. Podatci se generiraju na isti način kao u zadatku 2.\n",
    "\n",
    "**NB:** Pobrinite se da svi grafikoni budu generirani nad **usporedivim skupovima podataka**, na sljedeći način. Generirajte najprije svih 1000 primjera, podijelite ih na skupove za učenje i skupove za ispitivanje (dva skupa od po 500 primjera), zatim generirajte tri parova skupova za učenje i ispitivanje, svaki s različitom količinom šuma. Naposlijetku iz tih skupova izdvojite dodatne podskupove od $N=200$ primjera ($100$ za učenje i $100$ za ispitivanje) i $N=100$ primjera ($50$ za učenje i $50$ za ispitivanje)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Lab1-3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Q:*** Jesu li rezultati očekivani? Obrazložite.\n",
    "\n",
    "Rezultati su očekivani, ali ipak su dosta varijabilni radi nasumično odabranih vrijednosti. Sa povećanjem broja primjera se smanjuje razlika pogrešaka i jedna i druga si postaju veoma slične. Kako odstupanje raste, povećava se i generativna i ispitna pogreška. Lagani propad se objašnjava kao idealan polinom koji u tom trenutku pogađa gotovo sve točke pa je njegova pogreška minimalna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. a)\n",
    "U gornjim eksperimentima nismo koristili **regularizaciju**. Vratimo se najprije na primjer iz zadatka 1. Na primjerima iz tog zadatka izračunajte težine $\\mathbf{w}$ za polinomijalni regresijski model stupnja $d=3$ uz L2-regularizaciju (tzv. *ridge regression*), prema izrazu $\\mathbf{w}=(\\mathbf{\\Phi}^\\intercal\\mathbf{\\Phi}+\\lambda\\mathbf{I})^{-1}\\mathbf{\\Phi}^\\intercal\\mathbf{y}$. Napravite izračun težina za regularizacijske faktore $\\lambda=0$, $\\lambda=1$ i $\\lambda=10$ te usporedite dobivene težine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kojih je dimenzija matrica koju treba invertirati?\n",
    "\n",
    "**R:** 4x4 (3 značajke + 1 dummy vrijednost)\n",
    "\n",
    "**Q:** Po čemu se razlikuju dobivene težine i je li ta razlika očekivana? Obrazložite.\n",
    "\n",
    "**R:** Razlikuju se, što je veći lambda, to je veće odstupanje od klasičnog modela. Što je veća lambda, to je iznos bliže nuli. Ne dozvoljavaju se velike razlike u redovima veličina parametara kako model ne bi bio prenaučen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. b)\n",
    "Proučite klasu [`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge) iz modula [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model), koja implementira L2-regularizirani regresijski model. Parametar $\\alpha$ odgovara parametru $\\lambda$. Primijenite model na istim primjerima kao u prethodnom zadatku i ispišite težine $\\mathbf{w}$ (metode `coef_` i `intercept_`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Jesu li težine identične onima iz zadatka 4a? Ako nisu, objasnite zašto je to tako te popravite izračun u zadatku 4a tako da težine budu identične ovima u zadatku 4b.\n",
    "\n",
    "**R:** Težine su prvotno bile različite, a sada su jednake kada više ne regulariziramo $w_0$. Ridge ne regularizira w0 što se vidi i iz dobivenih koeficijenata $\\lambda$ (koeficijenti se ne snizuju zanačajno za redove veličine lambda 1->10->100..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. a)\n",
    "\n",
    "Vratimo se na slučaj $N=50$ slučajno generiranih primjera iz zadatka 2. Trenirajte modele polinomijalne regresije $\\mathcal{H}_{\\lambda,d}$ za $\\lambda\\in\\{0,100\\}$ i $d\\in\\{2,10\\}$ (ukupno četiri modela). Skicirajte pripadne funkcije $h(\\mathbf{x})$ i primjere (na jednom grafikonu; preporučujemo koristiti `plot` unutar `for` petlje)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Lab1-5a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Jesu li rezultati očekivani? Obrazložite.\n",
    "\n",
    "**R:** Regularizacija je smanjila valovitost modela\n",
    "\n",
    "**Q:** Za koji od ovih četiri modela biste očekivali da će najbolje generalizirati?\n",
    "\n",
    "**R:** model istog stupnja kao onaj na kojeg je dodan sum ? not sure :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. b)\n",
    "\n",
    "Kao u zadataku 3b, razdvojite primjere na skup za učenje i skup za ispitivanje u omjeru 1:1. Prikažite krivulje logaritama pogreške učenja i ispitne pogreške u ovisnosti za model $\\mathcal{H}_{d=20,\\lambda}$, podešavajući faktor regularizacije $\\lambda$ u rasponu $\\lambda\\in\\{0,1,\\dots,50\\}$. L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Lab1-5b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kojoj strani na grafikonu odgovara područje prenaučenosti a kojoj podnaučenosti. Zašto?\n",
    "\n",
    "**R:** Lijevo je graf prenaučen, dok je desno podnaučen. Razlog podnaučenosti je taj što veliki faktor potiskuje veliku složenost i varijabilnost modela te ga drži u određenim granicama. Mala pogreska ucenja i velika ispitna nam govori da je model previše adapriran šumu nastalome.\n",
    "\n",
    "**Q:** Koju biste vrijednosti za $\\lambda$ izabrali na temelju ovih grafikona i zašto?\n",
    "\n",
    "**R:** Vrijednost $\\lambda$ bih odabrao 14 jer tada dolazi do maksimuma ispitne pogreške i daljnjih povećavanjem $\\lambda$ parametra ne dolazimo do značajnih napredaka.\n",
    "\n",
    "**Q:** Jesu li rezultati stabilni? Zašto?\n",
    "\n",
    "**R:** Nisu, treniranje na relativno malom skupu primjera i nije moguća takva generalizaija, sve ovisi o odabiru testnih i trening setova."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. a)\n",
    "\n",
    "Za ovaj zadatak upotrijebite skup za učenje i skup za testiranje iz zadatka 3b. Trenirajte modele **L2-regularizirane** polinomijalne regresije stupnja $d=20$, mijenjajući hiperparametar $\\lambda$ u rasponu $\\{1,2,\\dots,100\\}$. Za svaki od treniranih modela izračunajte L{0,1,2}-norme vektora težina $\\mathbf{w}$ te ih prikažite kao funkciju od $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Lab1-6a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Objasnite oblik obiju krivulja. Hoće li krivulja za $\\|\\mathbf{w}\\|_2$ doseći nulu? Zašto? Je li to problem? Zašto?\n",
    "\n",
    "**R:** Težinu $w_0$ treba izuzeti iz regularizacijskog izraza (jer ona definira pomak) ili treba centrirati podatke tako da $\\overline{y}=0$, jer onda $w_0\\to0$ **(iz skripte nezz dal na to misle)**\n",
    "\n",
    "**Q:** Za $\\lambda=100$, koliki je postotak težina modela jednak nuli, odnosno koliko je model rijedak?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. b)\n",
    "Glavna prednost L1-regularizirane regresije (ili *LASSO regression*) nad L1-regulariziranom regresijom jest u tome što L1-regularizirana regresija rezultira **rijetkim modelima** (engl. *sparse models*), odnosno modelima kod kojih su **mnoge težine pritegnute na nulu**. Pokažite da je to doista tako, ponovivši gornji eksperiment s **L1-regulariziranom** regresijom, implementiranom u klasi  [`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html) u modulu [`sklearn.linear_model`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Lab1-6b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Kojem biste modelu (Ridge ili LASSO) u praksi dali prednost, i zašto? U kojim situacijama je ta prednost osobito izražena?\n",
    "\n",
    "**R:** Prednost bih dao Rigdge modelu jer minimizira 2. normu vektora, tj. njegovu apsolutnu duljinu, a ne duljinu po iznostima. Lasso model će prije poništiti više polinome, dok će Rigdje minimizirati njihovu ukupnu duljinu i držati vrijednosti probližno jednakima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. a)\n",
    "Do sada smo razmatrali isključivo univarijatnu regresiju, tj. imali smo samo jednu značajku ($n=1$). U većini stvarnih problema baratamo s većim brojem značajki. Razmotrimo sada jedan nešto realniji problem, kod kojega postoji više značajki, pa je potrebno napraviti multivarijatnu regresiju.\n",
    "\n",
    "Učitajte skup podataka *Boston House Prices*:\n",
    "\n",
    "Vaš je zadatak da izgradite regresijski model za predviđanje cijene nekretnine (`y=boston.target`) na temelju 13 raspoloživih značajki za svaku nekretninu (`X=boston.data`). Cilj je pronaći najbolji mogući linearni model regresije na ovom skupu podataka i provjeriti njegovu točnost u smislu pogreške kvadratnog odstupanja ([`mean_squared_error`]( http://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html#sklearn.metrics.mean_squared_error)). \n",
    "\n",
    "Hiperparametri modela koje treba isprobati su:\n",
    "\n",
    "* **Regularizacija:** Bez regularizacije ([`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression)), L2-regularizacija ([`Ridge`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)) i L1-regularizacija ([`Lasso`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso));\n",
    "* **Značajke:** Izvornih 13 značajki, polinomijalne značajke (isprobajte različite stupnjeve polinoma $d$), samo interakcijske značajke (opcija `interaction_only` u klasi [`PolynomialFeatures`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html));\n",
    "\n",
    "Kao i inače, za odabir i ispitivanje modela koristit ćemo **unakrsnu provjeru** (engl. *cross-validation*). Skup primjera za učenje podijelit ćemo na **skup za učenje**, **skup za provjeru** i **skup za ispitivanje** u omjeru (otprilike) 3:1:1. Kao u uvijek, model trebate trenirati na skupu za učenje, odabir modela (odnosno optimizaciju hiperparametra) trebate provesti na skupu za provjeru, a konačno vrednovanje modela trebate načiniti na skupu za ispitivanje. Konačno vrednovanje radite samo jednom, za model koji ste na skupu za provjeru odabrali kao optimalan.\n",
    "\n",
    "**NB:** Nakon što odaberete optimalan model na skupu za provjeru, prije konačnog ispitivanja odabrani model ponovno trenirajte na uniji skupova za učenje i provjeru. Na taj način iskorištavate maksimalno iskorištavate dostupne podatke i model će u pravilu biti bolji.\n",
    "\n",
    "Podjela na skup za učenje, provjeru i ispitivanje u ovom je slučaju fiksna kako bi svi imali identične skupove i kako bi rezultati bili usporedivi. (U stvarnosti biste ovakav eksperiment radili malo drugačije: koristili biste višestruku unakrsnu provjeru ili ugnježđenu unakrsnu provjeru. Više o tome u budućim vježbama.) \n",
    "\n",
    "Koristite sljedeće skupove:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q:** Koliko značajki ima svaki od modela koji ste isprobali?  \n",
    "\n",
    "**R:** (n+k-1 povrh k) +1 znacajka\n",
    "* n-> broj dimenzija x koji se polyfitom transformira\n",
    "* k-> stupanj polinoma\n",
    "\n",
    "**Q:** Provjerite točnost odabranog modela na (1) skupu za učenje, (2) skupu za provjeru, (3) uniji ta dva skupa i (4) skupu za ispitivanje. Jesu li odnosi između točnosti modela na ova četiri skupa očekivana? Obrazložite.\n",
    "\n",
    "**R:** Ocekivano tocnost na skupu za ucenje je velika jer je model treniran na tim podacima, zatim na uniji skupa za ucenje i skupa za provjeru je sljedeca najveca tocnost jer smo pomijesali podatke na kojima je model trenirao s onima koje smo koristili za optimiziranje hiperparametara. Zatim slijedi tocnost na skupu za provjeru i najmanja tocnost na skupu za ispitivanje jer te podatke model nikada nije ni vidio niti koristio za optimiziranje parametara\n",
    "\n",
    "**Q:** Kod treniranja regresijskog modela moguće je postaviti `fit_intercept=False`, čime se izbjegava optimiranje težine $w_0$. Trenirajte odabrani model s tom postavkom. Usporedite s točnošću optimalnog modela. Je li rezultat očekivan? Obrazložite. Ima li predobrada značajki ikakvog utjecaja na ovu razliku?\n",
    "\n",
    "**R:** kada fit_intercept stavimo na False model ocekuje standardizirane podatke na 0-1 odnosno nece koristiti bias (i guess?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
